# Hidden Markov models {#sec-HMM}

Hidden Markov Models are used for data for which 1) we believe that the distribution generating the observation depends on the state of an underlying, hidden state, and 2) the hidden states follow a Markov process, i.e., the states over time are not independent of one another, but the current state depends on the previous state only (and not on earlier states) [see e.g., @Rabiner1989; @ephraim2002; @cappe2005; @zucchini2016]. The HMM is a discrete time model: for each point in time $t$, we have one hidden state that generates one observed event for that time point $t$.\
Hence, the probability of observing the current outcome $O_t$ is exclusively determined by the current latent state $S_t$:

```{=tex}
\begin{equation}
Pr(O_{t} \mid \ O_{t-1}, O_{t-2}, \ldots, O_{1}, \ S_{t}, S_{t-1}, \ldots, S_{1}) = Pr(O_{t} \mid S_{t}).
\end{equation}
```
The probability of observing $O_t$ given $S_t$ can have any distribution, e.g., discrete or continuous. In the current version of the package `mHMMbayes`, only the categorical emission distribution is implemented.\
The hidden states in the sequence take values from a countable finite set of states $S_t = i, i \in \{1, 2, \ldots, m\}$, where $m$ denotes the number of distinct states, that form the Markov chain, with the Markov property:

```{=tex}
\begin{equation}
Pr(S_{t+1} \mid \ S_{t}, S_{t-1}, \ldots, S_{1}) = Pr(S_{t+1} \mid S_{t}).
\end{equation}
```
That is, the probability of switching to the next state $S_{t+1}$ depends only on the current state $S_t$. As the HMM is a discrete time model, the duration of a state is represented by the self-transition probabilities $\gamma_{ii}$, where the probability of a certain time t spent in state $S$ is given by the geometric distribution: $\gamma_{ii}^{t-1}(1-\gamma_{ii})$.\
The HMM includes three sets of parameters: the initial probabilities of the states $\pi_i$, the matrix $\mathbf{\Gamma}$ including the transition probabilities $\gamma_{ij}$ between the states, and the state-dependent probability distribution of observing $O_t$ given $S_t$ with parameter set $\boldsymbol{\theta}_i$. The initial probabilities $\pi_i$ denote the probability that the first state in the hidden state sequence, $S_1$, is $i$:

```{=tex}
\begin{equation}
\pi_i = Pr(S_1 = i) \quad \text{with} \sum_i \pi_i = 1. 
\end{equation}
```
Often, the initial probabilities of the states $\pi_i$ are assumed to be the stationary distribution implied by the transition probability matrix $\mathbf{\Gamma}$, that is, the long term steady-state probabilities obtained by $\lim_{T \rightarrow \infty} \mathbf{\Gamma}^T$. The transition probability matrix $\mathbf{\Gamma}$ with transition probabilities $\gamma_{ij}$ denote the probability of switching from state $i$ at time $t$ to state $j$ at time $t+1$:

```{=tex}
\begin{equation}
\gamma_{ij} = Pr(S_{t+1} = j \mid S_{t} = i) \quad \text{with} \sum_j \gamma_{ij} = 1.
\end{equation}
```
That is, the transition probabilities $\gamma_{ij}$ in the HMM represent the probability to switch between hidden states rather than between observed acts, as in the MC and CTMC model. The state-dependent probability distribution denotes the probability of observing $O_t$ given $S_t$ with parameter set $\boldsymbol{\theta}_i$. In case of the package, the state-dependent probability distribution is given by the categorical distribution, and the parameter set $\boldsymbol{\theta}_i$ is the set of state-dependent probabilities of observing categorical outcomes. That is,

```{=tex}
\begin{equation}
Pr(O_t = o \mid S_t = i) \sim \text{Cat} (\boldsymbol{\theta}_i),
\end{equation}
```
for the observed outcomes $o = 1, 2, \ldots, q$ and where $\boldsymbol{\theta}_i = (\theta_{i1}, \theta_{i2}, \ldots, \theta_{iq})$ is a vector of probabilities for each state $S = i, \ldots, m$ with $\sum \theta_i = 1$, i.e., within each state, the probabilities of all possible outcomes sum up to 1.\
We assume that all parameters in the HMM are independent of $t$, i.e., we assume a time-homogeneous model. In the vignette [Estimation of the multilevel hidden Markov model](https://cran.r-project.org/web/packages/mHMMbayes/vignettes/estimation-mhmm.pdf) we discuss three methods (i.e., Maximum likelihood, Expectation Maximization or Baum-Welch algorithm, and Bayesian estimation) to estimate the parameters of an HMM. In the package `mHMMbayes`, we chose to use Bayesian estimation because of its flexibility, which we will require in the multilevel framework of the model.
