[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Multilevel HMM Tutorial",
    "section": "",
    "text": "Abstract\nWith the R package mHMMbayes you can fit multilevel hidden Markov models. The multilevel hidden Markov model (HMM) is a generalization of the well-known hidden Markov model, tailored to accommodate (intense) longitudinal data of multiple individuals simultaneously. Using a multilevel framework, we allow for heterogeneity in the model parameters (transition probability matrix and conditional distribution), while estimating one overall HMM.\nThe model has a great potential of application in many fields, such as the social sciences and medicine. The model can be fitted on multivariate data with a categorical distribution, and include individual level covariates (allowing for e.g., group comparisons on model parameters). Parameters are estimated using Bayesian estimation utilizing the forward-backward recursion within a hybrid Metropolis within Gibbs sampler. The package also includes a function to simulate data and a function to obtain the most likely hidden state sequence for each individual using the Viterbi algorithm."
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "1  Preface",
    "section": "",
    "text": "Hidden Markov models [HMMs; Rabiner (1989)] are a machine learning method that have been used in many different scientific fields to describe a sequence of observations for several decades. For example, translating a fragment of spoken words into text (i.e., speech recognition, see e.g. Rabiner 1989; Woodland and Povey 2002), or the identification of the regions of DNA that encode genes (i.e., gene tagging, see e.g., Krogh, Mian, and Haussler 1994; Henderson, Salzberg, and Fasman 1997; Burge and Karlin 1998). The development of this package is, however, motivated from the area of social sciences. Due to technological advancements, it becomes increasingly easy to collect long sequences of data on behavior. That is, we can monitor behavior as it unfolds in real time. An example here of is the interaction between a therapist and a patient, where different types of nonverbal communication are registered every second for a period of 15 minutes. When applying HMMs to such behavioral data, they can be used to extract latent behavioral states over time, and model the dynamics of behavior over time.\n\nA quite recent development in HMMs is the extension to multilevel HMMs (see e.g., Altman 2007; Shirley et al. 2010; Rueda, Rueda, and Diaz-Uriarte 2013; Zhang and Berhane 2014; Haan-Rietdijk et al. 2017). Using the multilevel framework, we can model several sequences (e.g., sequences of different persons) simultaneously, while accommodating the heterogeneity between persons. As a result, we can quantify the amount of variation between persons in their dynamics of behavior, easily perform group comparisons on the model parameters, and investigate how model parameters change as a result of a covariate. For example, are the dynamics between a patient and a therapist different for patients with a good therapeutic outcome and patients with a less favorable therapeutic outcome?\n\nWith the package mHMMbayes, one can estimate these multilevel hidden Markov models. This tutorial starts out with a brief description of the HMM and the multilevel HMM. For a more elaborate and gentle introduction to HMMs, we refer to Zucchini, MacDonald, and Langrock (2016). Next, we show how to use the package mHMMbayes through an extensive example, also touching on the issues of determining the number of hidden states and checking model convergence. Information on the used estimation methods and algorithms in the package is given in the vignette Estimation of the multilevel hidden Markov model.\n\n\n\n\nAltman, Rachel MacKay. 2007. “Mixed Hidden Markov Models: An Extension of the Hidden Markov Model to the Longitudinal Data Setting.” Journal of the American Statistical Association 102 (477): 201–10.\n\n\nBurge, Christopher B, and Samuel Karlin. 1998. “Finding the Genes in Genomic DNA.” Current Opinion in Structural Biology 8 (3): 346–54.\n\n\nHaan-Rietdijk, S de, Peter Kuppens, Cindy S Bergeman, LB Sheeber, NB Allen, and EL Hamaker. 2017. “On the Use of Mixed Markov Models for Intensive Longitudinal Data.” Multivariate Behavioral Research 52 (6): 747–67.\n\n\nHenderson, John, Steven Salzberg, and Kenneth H Fasman. 1997. “Finding Genes in DNA with a Hidden Markov Model.” Journal of Computational Biology 4 (2): 127–41.\n\n\nKrogh, Anders, I Saira Mian, and David Haussler. 1994. “A Hidden Markov Model That Finds Genes in e. Coli DNA.” Nucleic Acids Research 22 (22): 4768–78.\n\n\nRabiner, Lawrence R. 1989. “A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition.” Proceedings of the IEEE 77 (2): 257–86.\n\n\nRueda, Oscar M, Cristina Rueda, and Ramon Diaz-Uriarte. 2013. “A Bayesian HMM with Random Effects and an Unknown Number of States for DNA Copy Number Analysis.” Journal of Statistical Computation and Simulation 83 (1): 82–96.\n\n\nShirley, Kenneth E, Dylan S Small, Kevin G Lynch, Stephen A Maisto, and David W Oslin. 2010. “Hidden Markov Models for Alcoholism Treatment Trial Data.” The Annals of Applied Statistics, 366–95.\n\n\nWoodland, Philip C, and Daniel Povey. 2002. “Large Scale Discriminative Training of Hidden Markov Models for Speech Recognition.” Computer Speech & Language 16 (1): 25–47.\n\n\nZhang, Yue, and Kiros Berhane. 2014. “Bayesian Mixed Hidden Markov Models: A Multi-Level Approach to Modeling Categorical Outcomes with Differential Misclassification.” Statistics in Medicine 33 (8): 1395–1408.\n\n\nZucchini, Walter, Iain L MacDonald, and Roland Langrock. 2016. Hidden Markov Models for Time Series: An Introduction Using R. Boca Raton: CRC Press."
  },
  {
    "objectID": "HMM.html",
    "href": "HMM.html",
    "title": "2  Hidden Markov models",
    "section": "",
    "text": "Hidden Markov Models are used for data for which 1) we believe that the distribution generating the observation depends on the state of an underlying, hidden state, and 2) the hidden states follow a Markov process, i.e., the states over time are not independent of one another, but the current state depends on the previous state only (and not on earlier states) (see e.g., Rabiner 1989; Ephraim and Merhav 2002; Cappé 2005; Zucchini, MacDonald, and Langrock 2016). The HMM is a discrete time model: for each point in time \\(t\\), we have one hidden state that generates one observed event for that time point \\(t\\).\nHence, the probability of observing the current outcome \\(O_t\\) is exclusively determined by the current latent state \\(S_t\\):\n\\[\\begin{equation}\nPr(O_{t} \\mid \\ O_{t-1}, O_{t-2}, \\ldots, O_{1}, \\ S_{t}, S_{t-1}, \\ldots, S_{1}) = Pr(O_{t} \\mid S_{t}).\n\\end{equation}\\]\nThe probability of observing \\(O_t\\) given \\(S_t\\) can have any distribution, e.g., discrete or continuous. In the current version of the package mHMMbayes, only the categorical emission distribution is implemented.\nThe hidden states in the sequence take values from a countable finite set of states \\(S_t = i, i \\in \\{1, 2, \\ldots, m\\}\\), where \\(m\\) denotes the number of distinct states, that form the Markov chain, with the Markov property:\n\\[\\begin{equation}\nPr(S_{t+1} \\mid \\ S_{t}, S_{t-1}, \\ldots, S_{1}) = Pr(S_{t+1} \\mid S_{t}).\n\\end{equation}\\]\nThat is, the probability of switching to the next state \\(S_{t+1}\\) depends only on the current state \\(S_t\\). As the HMM is a discrete time model, the duration of a state is represented by the self-transition probabilities \\(\\gamma_{ii}\\), where the probability of a certain time t spent in state \\(S\\) is given by the geometric distribution: \\(\\gamma_{ii}^{t-1}(1-\\gamma_{ii})\\).\nThe HMM includes three sets of parameters: the initial probabilities of the states \\(\\pi_i\\), the matrix \\(\\mathbf{\\Gamma}\\) including the transition probabilities \\(\\gamma_{ij}\\) between the states, and the state-dependent probability distribution of observing \\(O_t\\) given \\(S_t\\) with parameter set \\(\\boldsymbol{\\theta}_i\\). The initial probabilities \\(\\pi_i\\) denote the probability that the first state in the hidden state sequence, \\(S_1\\), is \\(i\\):\n\\[\\begin{equation}\n\\pi_i = Pr(S_1 = i) \\quad \\text{with} \\sum_i \\pi_i = 1.\n\\end{equation}\\]\nOften, the initial probabilities of the states \\(\\pi_i\\) are assumed to be the stationary distribution implied by the transition probability matrix \\(\\mathbf{\\Gamma}\\), that is, the long term steady-state probabilities obtained by \\(\\lim_{T \\rightarrow \\infty} \\mathbf{\\Gamma}^T\\). The transition probability matrix \\(\\mathbf{\\Gamma}\\) with transition probabilities \\(\\gamma_{ij}\\) denote the probability of switching from state \\(i\\) at time \\(t\\) to state \\(j\\) at time \\(t+1\\):\n\\[\\begin{equation}\n\\gamma_{ij} = Pr(S_{t+1} = j \\mid S_{t} = i) \\quad \\text{with} \\sum_j \\gamma_{ij} = 1.\n\\end{equation}\\]\nThat is, the transition probabilities \\(\\gamma_{ij}\\) in the HMM represent the probability to switch between hidden states rather than between observed acts, as in the MC and CTMC model. The state-dependent probability distribution denotes the probability of observing \\(O_t\\) given \\(S_t\\) with parameter set \\(\\boldsymbol{\\theta}_i\\). In case of the package, the state-dependent probability distribution is given by the categorical distribution, and the parameter set \\(\\boldsymbol{\\theta}_i\\) is the set of state-dependent probabilities of observing categorical outcomes. That is,\n\\[\\begin{equation}\nPr(O_t = o \\mid S_t = i) \\sim \\text{Cat} (\\boldsymbol{\\theta}_i),\n\\end{equation}\\]\nfor the observed outcomes \\(o = 1, 2, \\ldots, q\\) and where \\(\\boldsymbol{\\theta}_i = (\\theta_{i1}, \\theta_{i2}, \\ldots, \\theta_{iq})\\) is a vector of probabilities for each state \\(S = i, \\ldots, m\\) with \\(\\sum \\theta_i = 1\\), i.e., within each state, the probabilities of all possible outcomes sum up to 1.\nWe assume that all parameters in the HMM are independent of \\(t\\), i.e., we assume a time-homogeneous model. In the vignette Estimation of the multilevel hidden Markov model we discuss three methods (i.e., Maximum likelihood, Expectation Maximization or Baum-Welch algorithm, and Bayesian estimation) to estimate the parameters of an HMM. In the package mHMMbayes, we chose to use Bayesian estimation because of its flexibility, which we will require in the multilevel framework of the model.\n\n\n\n\nCappé, E. AND Rydén, O. AND Moulines. 2005. Inference in Hidden Markov Models. New York: Springer.\n\n\nEphraim, Yariv, and Neri Merhav. 2002. “Hidden Markov Processes.” Information Theory, IEEE Transactions on 48 (6): 1518–69.\n\n\nRabiner, Lawrence R. 1989. “A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition.” Proceedings of the IEEE 77 (2): 257–86.\n\n\nZucchini, Walter, Iain L MacDonald, and Roland Langrock. 2016. Hidden Markov Models for Time Series: An Introduction Using R. Boca Raton: CRC Press."
  },
  {
    "objectID": "mHMM.html",
    "href": "mHMM.html",
    "title": "3  Multilevel hidden Markov models",
    "section": "",
    "text": "Given data of multiple subjects, one may fit the HMM to the data of each subject separately, or fit one and the same HMM model to the data of all subject, under the strong (generally untenable) assumption that the subjects do not differ with respect to the parameters of the HMM. Fitting a different model to each behavioral sequence is not parsimonious, computationally intensive, and results in a large number of parameters estimates. Neither approach lends itself well for a formal comparison (e.g., comparing the parameters over experimental conditions). To facilitate the analysis of multiple subjects, the HMM is extended by putting it in a multilevel framework.\n\nIn multilevel models, model parameters are specified that pertain to different levels in the data. For example, subject-specific model parameters describe the data collected within each subject, and group level parameters describe what is typically observed within the group of subjects, and the variation observed between subjects. In the implemented multilevel HMM, we allow each subject to have its own unique parameter values within the same HMM model (i.e., identical number and similar composition of the hidden states). Rather than estimating these subject-specific parameters individually, we assume that the parameters of the HMM are random, i.e., follow a given group level distribution. Within this multilevel structure, the mean and the variance of the group level distribution of a given parameter thus expresses the overall mean parameter value in a group of subjects and the parameter variability between the subjects in the group.\n\nMultilevel HMMs have received some attention in the literature. In a frequentist context, Altman (2007) presented a general framework for HMMs for multiple processes by defining a class of Mixed Hidden Markov Models (MHMMs). These models are however, computationally intensive and due to slow convergence only suited for modeling a limited number of random effects. The approach of Altman has been translated to the Bayesian framework, which proved much faster as the time to reach convergence is decreased Zhang and Berhane (2014). In addition, the HMM in a Bayesian context is easier to adapt to a multilevel model, as the need for numerical integration is eliminated. Examples of the application of the multilevel HMM (within a Bayesian framework) are: Rueda, Rueda, and Diaz-Uriarte (2013) applied the model to the analysis of DNA copy number data, Zhang and Berhane (2014) to identify risk factors for asthma, Shirley et al. (2010) to clinical trial data of a treatment for alcoholism and Haan-Rietdijk et al. (2017) to longitudinal data sets in psychology.\n\nIn the tutorial, we use the following notation for the parameters in the multilevel HMM. The subject specific parameters are supplemented with the prefix \\(k\\), denoting subject \\(k \\in \\{1,2,\\ldots,K\\}\\). Hence, in the multilevel (Bayesian) HMM, the subject specific parameters are:\n\nthe subject-specific transition probability matrix \\(\\boldsymbol{\\Gamma}_k\\) with transition probabilities \\(\\gamma_{k,ij}\\)\nthe subject-specific emission distributions denoting subject-specific probabilities. \\(\\boldsymbol{\\theta}_{k,i}\\) of categorical outcomes within hidden state \\(i\\).\n\nThe group level parameters are:\n\nthe group level state transition probability matrix \\(\\boldsymbol{\\Gamma}\\) with transition probabilities \\(\\gamma_{ij}\\).\nthe group level state-dependent probabilities \\(\\boldsymbol{\\theta}_{i}\\).\n\n\n\n\n\n\n\nNote\n\n\n\nThe initial probabilities of the states \\(\\pi_{k,j}\\) are not estimated as \\(\\pi_{k}\\) is assumed to be the stationary distribution of \\(\\boldsymbol{\\Gamma}_k\\).\n\n\nWe fit the model using Bayesian estimation (i.e., a hybrid Metropolis Gibbs sampler that utilizes the forward-backward recursion to sample the hidden state sequence of each subject, see the vignette Estimation of the multilevel hidden Markov model).\n\n\n\n\nAltman, Rachel MacKay. 2007. “Mixed Hidden Markov Models: An Extension of the Hidden Markov Model to the Longitudinal Data Setting.” Journal of the American Statistical Association 102 (477): 201–10.\n\n\nHaan-Rietdijk, S de, Peter Kuppens, Cindy S Bergeman, LB Sheeber, NB Allen, and EL Hamaker. 2017. “On the Use of Mixed Markov Models for Intensive Longitudinal Data.” Multivariate Behavioral Research 52 (6): 747–67.\n\n\nRueda, Oscar M, Cristina Rueda, and Ramon Diaz-Uriarte. 2013. “A Bayesian HMM with Random Effects and an Unknown Number of States for DNA Copy Number Analysis.” Journal of Statistical Computation and Simulation 83 (1): 82–96.\n\n\nShirley, Kenneth E, Dylan S Small, Kevin G Lynch, Stephen A Maisto, and David W Oslin. 2010. “Hidden Markov Models for Alcoholism Treatment Trial Data.” The Annals of Applied Statistics, 366–95.\n\n\nZhang, Yue, and Kiros Berhane. 2014. “Bayesian Mixed Hidden Markov Models: A Multi-Level Approach to Modeling Categorical Outcomes with Differential Misclassification.” Statistics in Medicine 33 (8): 1395–1408."
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "4  Example data",
    "section": "",
    "text": "We illustrate using the mHMMbayes package using the embedded example data nonverbal. The data contains the nonverbal communication of 10 patient-therapist couples, recorded for 15 minutes at a frequency of 1 observation per second (= 900 observations per couple).\nThe following variables are contained in the dataset:\n\n\nid: id variable of patient - therapist couple to distinguish which observation belongs to which couple.\n\np_verbalizing: verbalizing behavior of the patient, consisting of 1 = not verbalizing, 2 = verbalizing, 3 = back channeling.\n\np_looking: looking behavior of the patient, consisting of 1 = not looking at therapist, 2 = looking at therapist.\n\nt_verbalizing: verbalizing behavior of the therapist, consisting of 1 = not verbalizing, 2 = verbalizing, 3 = back channeling.\n\nt_looking: looking behavior of the therapist, consisting of 1 = not looking at patient, 2 = looking at patient. The top 6 rows of the dataset are provided below.\n\n\nBelow is a glimpse of how the data appears.\n\n\n\n\n#>   id p_vocalizing p_looking t_vocalizing t_looking\n#> 1  1            2         2            2         2\n#> 2  1            2         2            1         2\n#> 3  1            2         2            2         2\n#> 4  1            2         2            2         2\n#> 5  1            2         2            2         2\n#> 6  1            2         2            1         2\n\n\n\nWhen we plot the data of the first 5 minutes (= the first 300 observations) of the first couple, we get the following:\n\n\n\n\n\n\n\n\nWe can, for example, observe that both the patient and the therapist are mainly looking at each other during the observed 5 minutes. During the first minute, the patient is primarily speaking. During the second minute, the therapists starts, after which the patient takes over while the therapist is back channeling."
  },
  {
    "objectID": "specification.html",
    "href": "specification.html",
    "title": "5  Model specification",
    "section": "",
    "text": "Let’s proceed with fitting a simple 2-state multilevel model using the mHMM function. First, we need to set some general model properties and starting values:\n\n# Load the mHMMbayes package \nlibrary(mHMMbayes)\n\n# Specify general model properties:\nm <- 2\nn_dep <- 4\nq_emiss <- c(3, 2, 3, 2)\n\n# Specify starting values for transition matrix (TM) and emission probabilities (EM)\nstart_TM <- diag(.8, m)\nstart_TM[lower.tri(start_TM) | upper.tri(start_TM)] <- .2\nstart_EM <- list(matrix(c(0.05, 0.90, 0.05, \n                          0.90, 0.05, 0.05), byrow = TRUE,\n                         nrow = m, ncol = q_emiss[1]), # vocalizing patient\n                  matrix(c(0.1, 0.9, \n                           0.1, 0.9), byrow = TRUE, nrow = m,\n                         ncol = q_emiss[2]), # looking patient\n                  matrix(c(0.90, 0.05, 0.05, \n                           0.05, 0.90, 0.05), byrow = TRUE,\n                         nrow = m, ncol = q_emiss[3]), # vocalizing therapist\n                  matrix(c(0.1, 0.9, \n                           0.1, 0.9), byrow = TRUE, nrow = m,\n                         ncol = q_emiss[4])) # looking therapist\n\nThe first line of code loads the mHMMbayes package and the nonverbal data.\nThen, we specify the general model properties:\n\nthe number of states used is set by m <- 2.\nthe number of dependent variables in the dataset used to infer the hidden states is specified by n_dep <- 4.\nthe number of categorical outcomes for each of the dependent variables is specified by q_emiss <- c(3, 2, 3, 2).\n\nThe subsequent lines of code specify the starting values for the transition probability matrix (start_TM) and the emission distributions (start_EM). Next, we will delve into more detail on how to specify these starting values."
  },
  {
    "objectID": "startingval.html",
    "href": "startingval.html",
    "title": "6  Starting values",
    "section": "",
    "text": "Previously in Chapter 4, we specified the starting values for the transition probability matrix (start_TM) and the emission distributions (start_EM), which are given to the model in the argument start_val (see the code snippet provided in Chapter 7).\nThese starting values are used for the first run of the forward backward algorithm. Although the hidden states cannot be observed, one often has an idea for probable compositions of the states.\nIn this example, we expect that there is a state in which the patient mostly speaks, and the therapist is silent, and a state during which the patient is silent and the therapists speaks. In addition, we expect that during both states, the therapist and the patient will be mainly looking at each other instead of looking away. Below in Table 6.1, we illustrate how these expectations can be specified.\n\n\nTable 6.1: Starting values for emission distributions\n\n\n\n\n(a) vocalizing patient\n\n\n\nNot Speaking\nSpeaking\nBack channeling\n\n\n\n\nState1\n0.05\n0.90\n0.05\n\n\nState2\n0.90\n0.05\n0.05\n\n\n\n\n\n\n(b) vocalizing therapist\n\n\n\nNot Speaking\nSpeaking\nBack channeling\n\n\n\n\nState1\n0.90\n0.05\n0.05\n\n\nState2\n0.05\n0.90\n0.05\n\n\n\n\n\n\n\n\n(c) looking patient\n\n\n\nNot looking\nLooking\n\n\n\n\nState1\n0.1\n0.9\n\n\nState2\n0.1\n0.9\n\n\n\n\n\n\n(d) looking therapist\n\n\n\nNot looking\nLooking\n\n\n\n\nState1\n0.1\n0.9\n\n\nState2\n0.1\n0.9\n\n\n\n\n\n\nOne usually also has some (vague) idea on likely and unlikely switches between states, and the size of self-transition probabilities. In this example, we think a state will usually last quite some seconds, and thus expect a rather high self-transition probability. Below in Table 6.2, we illustrate how these expectations can be specified.\n\n\nTable 6.2: Starting values for transition probabilities\n\n\n\nState1\nState2\n\n\n\n\nState1\n0.8\n0.2\n\n\nState2\n0.2\n0.8\n\n\n\n\nAll these ideas can be used to construct a set of sensible starting values. Using sensible starting values increases convergence speed, and often prevents a problem called label switching. Hence, using random or uniform starting values is not recommended, and a default option to do so is not included in the package.\n\n\n\n\n\n\nNote\n\n\n\nNote that it is strongly advised to check model convergence and label switching. That is, one should check if the algorithm reaches the same solution when a set of different (but often conceptually similar) starting values are used, and if label switching is not a problem. See Chapter 14 for an example. See the vignette Estimation of the multilevel hidden Markov model for more information on the forward backward algorithm and on the problem of label switching."
  },
  {
    "objectID": "prior.html#how-to-specify-prior-distributions",
    "href": "prior.html#how-to-specify-prior-distributions",
    "title": "7  Prior distributions",
    "section": "\n7.1 How to specify prior distributions",
    "text": "7.1 How to specify prior distributions\nTo specify user specific prior distributions, one uses the input option emiss_hyp_prior for the emission distribution and gamma_hyp_prior for the transition probabilities in the function mHMM. These input arguments take an object from the class mHMM_prior_emiss and mHMM_prior_gamma created by the functions prior_emiss_cat and prior_gamma, respectively. Both objects are a list, containing the following key elements:\n\n\nmu0, a lists containing the hypothesized hyper-prior mean values of the intercepts of the Multinomial logit model.\n\nK0, the number of hypothetical prior subjects on which the set of hyper-prior mean intercepts specified in mu0 are based.\n\nnu, degrees of freedom of the hyper-prior Inverse Wishart distribution on the covariance of the Multinomial logit intercepts.\n\nV, the variance-covariance of the hyper-prior Inverse Wishart distribution on the covariance of the Multinomial logit intercepts.\n\nNote that K0, nu and V are assumed equal over the states. The mean values of the intercepts (and regression coefficients of the covariates) denoted by mu0 are allowed to vary over the states. All elements in the list either have the prefix gamma_ or emiss_, depending on which list they belong to.\n\n\n\n\n\n\nCaution\n\n\n\nWhen specifying prior distributions, note that the first element of each row in the probability domain does not have an intercept, as it serves as baseline category in the Multinomial logit regression model.\nThis means, for example, that if we would specify a model with 3 states, mu0 is a vector with 2 elements, K0 and nu contain 1 element and V is a 2 by 2 matrix.\n\n\n\n7.1.1 Hyper-prior on transition probability matrix\nWhen using the prior_gamma function to specify informative hyper-prior on gamma, and if the hyper-prior values for gamma_K0, gamma_nu and gamma_V are not manually specified, the default values are as follows:\n\n\ngamma_K0 is set to \\(1\\).\n\ngamma_nu is set to \\(3 + m- 1\\).\nthe diagonal elements of gamma_V (i.e., the variance) are set to \\(3 + m - 1\\), while the off-diagonal elements (i.e., the covariance) are set to 0.\n\nThus, the user only needs to specify the number of states m and the hypothesized hyper-prior mean values of the multinomial logit intercepts, as default values are available for all other hyper-prior distribution parameters. The following code snippet provides an example of how to specify these values.\n\n# 1. specify general model properties\nm <- 3\n\n# 2. represent a prior belief: switching to state 3 does not occur often \n# and state 3 has a relative short duration\nprior_prob_gamma <- matrix(c(0.70, 0.25, 0.05,\n                             0.25, 0.70, 0.05,\n                             0.30, 0.30, 0.40), nrow = m, ncol = m, byrow = TRUE)\n\n# 3. use the function prob_to_int() to obtain intercept values for the \n# above specified transition probability matrix gamma\nprior_int_gamma <- prob_to_int(prior_prob_gamma)\ngamma_mu0 <- list(matrix(prior_int_gamma[1,], nrow = 1, ncol = m-1),\n                  matrix(prior_int_gamma[2,], nrow = 1, ncol = m-1),\n                  matrix(prior_int_gamma[3,], nrow = 1, ncol = m-1))\n\n# 4. feed them to the function prior_gamma()\nmanual_prior_gamma <- prior_gamma(m = m, gamma_mu0 = gamma_mu0)\n\n# 5. use the informative hyper-prior when fitting a model using the function mHHM()\n# mHHM(s_data = ..., ..., gamma_hyp_prior = manual_prior_gamma)\n\n\n\n\n\n\n\nNote\n\n\n\nWhen no manual values for the hyper-prior on gamma are specified at all (that is, the function prior_gamma is not used), all elements of the matrices contained in gamma_mu0 are set to 0 in the function mHMM.\n\n\n\n7.1.2 Hyper-prior on categorical emission distribution(s)\nWhen using the prior_emiss_cat function to specify informative hyper-prior on the categorical emission distirbution(s), and if the hyper-prior values for emiss_K0, emiss_nu and emiss_V are not manually specified, the default values are as follows:\n\n\nemiss_K0 is set to \\(1\\).\n\nemiss_nu is set to \\(3 + q\\_emiss[k] - 1\\).\nthe diagonal elements of emiss_V (i.e., the variance) are set to \\(3 + q\\_emiss[k] - 1\\), while the off-diagonal elements (i.e., the covariance) are set to 0.\n\nSimilarly to the transition probabilities above, if no manual values for the hyper-prior on the categorical emission distribution are specified at all (i.e., the function prior_emiss_cat is not used), all elements of the matrices contained in emiss_mu0 are set to 0 in the function mHMM. The following example code demonstrates how to specify the hyper-prior on emission distributions using the prior_emiss_cat function.\n\n# 1. specify general model properties\nm <- 3\nn_dep <- 4\nq_emiss <- c(3, 2, 3, 2)\n\n# 2. specify hypothesized mean emission probabilities\nprior_prob_emiss_cat <- list(matrix(c(0.10, 0.80, 0.10,\n                                      0.80, 0.10, 0.10,\n                                      0.40, 0.40, 0.20), byrow = TRUE,\n                                    nrow = m, ncol = q_emiss[1]), # vocalizing patient,\n                             # prior belief: state 1 - much talking, state 2 -\n                             # no talking, state 3 - mixed\n                             matrix(c(0.30, 0.70,\n                                      0.30, 0.70,\n                                      0.30, 0.70), byrow = TRUE, nrow = m,\n                                    ncol = q_emiss[2]), # looking patient\n                             # prior belief: all 3 states show frequent looking\n                             # behavior\n                             matrix(c(0.80, 0.10, 0.10,\n                                      0.10, 0.80, 0.10,\n                                      0.40, 0.40, 0.20), byrow = TRUE,\n                                    nrow = m, ncol = q_emiss[3]), # vocalizing therapist\n                             # prior belief: state 1 - no talking, state 2 -\n                             # frequent talking, state 3 - mixed\n                             matrix(c(0.30, 0.70,\n                                      0.30, 0.70,\n                                      0.30, 0.70), byrow = TRUE, nrow = m,\n                                    ncol = q_emiss[4])) # looking therapist\n# prior belief: all 3 states show frequent looking\n# behavior\n\n# 3. use the function prob_to_int() to obtain intercept values for the above specified\n# categorical emission distributions\nprior_int_emiss <- sapply(prior_prob_emiss_cat, prob_to_int)\nemiss_mu0 <- rep(list(vector(mode = \"list\", length = m)), n_dep)\nfor(k in 1:n_dep){\n  for(i in 1:m){\n    emiss_mu0[[k]][[i]] <- matrix(prior_int_emiss[[k]][i,], nrow = 1)\n  }\n}\n\n# 4. feed them to the function prior_emiss_cat()\nmanual_prior_emiss <- prior_emiss_cat(gen = list(m = m, n_dep = n_dep, q_emiss = q_emiss),\n                                      emiss_mu0 = emiss_mu0)\n\n# 5. use the informative hyper-prior when fitting a model using the function mHHM()\n# mHHM(s_data = ..., ..., emiss_hyp_prior = manual_prior_emiss)\n\n\n\n\n\n\n\nNote\n\n\n\nIn case covariates are specified, the hyper-prior parameter values of the inverse Wishart distribution on the covariance matrix remain unchanged, as the estimates of the regression coefficients for the covariates are fixed over subjects."
  },
  {
    "objectID": "fittingmodel.html",
    "href": "fittingmodel.html",
    "title": "8  Fitting the model",
    "section": "",
    "text": "The multilevel Hidden Markov Model (HMM) is fitted using the function mHMM. Below, we demonstrate how to fit a model without any covariates and with default priors.\n\n# Run a model without covariate(s) and with default priors\nset.seed(14532)\nout_2st <- mHMM(s_data = nonverbal, \n                    gen = list(m = m, n_dep = n_dep, q_emiss = q_emiss), \n                    start_val = c(list(start_TM), start_EM),\n                    mcmc = list(J = 1000, burn_in = 200))\n\nThe call to mHMM specifies the model with several arguments.\n\ns_data argument specifies the input data used to infer the hidden states over time.\ngen and start_val arguments specify the general model properties and the starting values, as discussed previously in Chapter 5 and Chapter 6.\n\nThe arguments needed for the MCMC algorithm are given in mcmc:\n\n\nJ specifies the number of iterations used by the hybrid metropolis within Gibbs algorithm.\n\nburn_in specifies the number of iterations to discard when obtaining the model parameter summary statistics.\n\n\n\nThe function mHMM returns an object of class mHMM, and you can use the print and summary methods to view the results. The print method provides essential information about the fitted model, including the number of subjects in the dataset, the number of iterations performed, the burn-in period, the average log likelihood across all subjects, and model fit indices such as AIC. Additionally, it displays the number of states specified and the number of dependent variables used for modeling the states. The information displayed is as follow.\n\nprint(out_2st)\n\n#> Number of subjects: 10 \n#> \n#> 1000 iterations used in the MCMC algorithm with a burn in of 200 \n#> Average Log likelihood over all subjects: -1624.389 \n#> Average AIC over all subjects: 3276.777 \n#> \n#> Number of states used: 2 \n#> \n#> Number of dependent variables used: 4\n\n\nThe summary method provides information on the estimated parameters. That is, the point estimates of the posterior distribution for the transition probability matrix and the emission distribution of each of the dependent variables at the group level, as shown below.\n\nsummary(out_2st)\n\n#> State transition probability matrix \n#>  (at the group level): \n#>  \n#>              To state 1 To state 2\n#> From state 1      0.929      0.071\n#> From state 2      0.074      0.926\n#> \n#> Emission distribution for each of the dependent variables \n#>  (at the group level): \n#>  \n#> $p_vocalizing\n#>         Category 1 Category 2 Category 3\n#> State 1      0.018      0.957      0.024\n#> State 2      0.795      0.052      0.153\n#> \n#> $p_looking\n#>         Category 1 Category 2\n#> State 1      0.248      0.752\n#> State 2      0.096      0.904\n#> \n#> $t_vocalizing\n#>         Category 1 Category 2 Category 3\n#> State 1      0.806      0.075      0.119\n#> State 2      0.034      0.945      0.020\n#> \n#> $t_looking\n#>         Category 1 Category 2\n#> State 1      0.047      0.953\n#> State 2      0.277      0.723\n\n\nThe resulting model indicates two well-separated states: one where the patient is speaking and another where the therapist is speaking. The looking behavior is quite similar for both the patient and the therapist in these two states. To obtain information on the estimated parameters, you can also use the functions obtain_gamma and obtain_emiss. These functions not only allow you to inspect the estimated parameters at the group level but also provide individual subject-level information by specifying the input variable level = \"subject\":\n\n# When not specified, level defaults to \"group\"\ngamma_pop <- obtain_gamma(out_2st)\ngamma_pop\n\n#>              To state 1 To state 2\n#> From state 1      0.929      0.071\n#> From state 2      0.074      0.926\n\n\n\n# To obtain the subject specific parameter estimates:\ngamma_subj <- obtain_gamma(out_2st, level = \"subject\")\ngamma_subj\n\n\n\n\n\n\n\nSee the list of subject-specific parameter estimates: gamma_subj\n\n\n\n\n\n\n\n#> $`Subject 1`\n#>              To state 1 To state 2\n#> From state 1      0.942      0.058\n#> From state 2      0.048      0.952\n#> \n#> $`Subject 2`\n#>              To state 1 To state 2\n#> From state 1      0.936      0.064\n#> From state 2      0.060      0.940\n#> \n#> $`Subject 3`\n#>              To state 1 To state 2\n#> From state 1      0.969      0.031\n#> From state 2      0.054      0.946\n#> \n#> $`Subject 4`\n#>              To state 1 To state 2\n#> From state 1      0.934      0.066\n#> From state 2      0.046      0.954\n#> \n#> $`Subject 5`\n#>              To state 1 To state 2\n#> From state 1      0.942      0.058\n#> From state 2      0.058      0.942\n#> \n#> $`Subject 6`\n#>              To state 1 To state 2\n#> From state 1      0.942      0.058\n#> From state 2      0.087      0.913\n#> \n#> $`Subject 7`\n#>              To state 1 To state 2\n#> From state 1      0.929      0.071\n#> From state 2      0.043      0.958\n#> \n#> $`Subject 8`\n#>              To state 1 To state 2\n#> From state 1       0.93      0.070\n#> From state 2       0.08      0.919\n#> \n#> $`Subject 9`\n#>              To state 1 To state 2\n#> From state 1      0.948      0.052\n#> From state 2      0.058      0.942\n#> \n#> $`Subject 10`\n#>              To state 1 To state 2\n#> From state 1      0.960      0.040\n#> From state 2      0.068      0.932\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nAn additional option provided by the functions obtain_gamma and obtain_emiss is the ability to change the burn-in period used for obtaining the summary statistics. This can be done by specifying the input argument burn_in."
  },
  {
    "objectID": "simulate.html#basics-of-simulating-data",
    "href": "simulate.html#basics-of-simulating-data",
    "title": "9  Simulating data",
    "section": "\n9.1 Basics of simulating data",
    "text": "9.1 Basics of simulating data\nBelow, we provide several examples demonstrating the usage of sim_mHHMM. Some basic arguments you need to specify are as follows:\n\nn_t: Numeric vector with length 1 denoting the length of the observed sequence to be simulated for each subject. To only simulate subject specific transition probability matrices gamma and emission distributions (and no data), set t to 0.\nn: Numeric vector with length 1 denoting the number of subjects for which data is simulated.\ngen: List containing the following elements denoting the general model properties (e.g., m, n_dep, q_emiss).\ngamma: A m by m matrix containing the average population transition probability matrix used for simulating the data. That is, the probability to switch from hidden state \\(i\\) to hidden state \\(j\\).\nemiss_distr: A list with n_dep elements containing the average population emission distribution(s) of the observations given the hidden states for each of the dependent variables. Each element is a matrix with m rows and q_emiss[k] columns for each of the k in n_dep emission distribution(s). That is, the probability of observing category \\(q\\) in state \\(i\\).\nvar_gamma: A numeric vector with length 1 denoting the amount of variance between subjects in the transition probability matrix.\nvar_emiss: A numeric vector with length n_dep denoting the amount of variance between subjects in the emission distribution(s).\n\n\n\n\n\n\n\nNote\n\n\n\nNote that var_gamma/var_emiss value corresponds to the variance of the parameters of the Multinomial distribution (i.e., the intercepts of the regression equation of the Multinomial distribution used to sample the transition probability matrix). Only one variance value can be specified for the complete transition probability matrix / emission distribution, hence the variance is assumed fixed across all components. The default equals 0.1, which corresponds to little variation between subjects. If one wants to simulate data from exactly the same HMM for all subjects, var_gamma/var_emiss should be set to 0. Note that if data for only 1 subject is simulated (i.e., n = 1), var_gamma/var_emiss is set to 0.\n\n\n\n## 1. Simulating data for 10 subjects with each 100 observations\n# number of observations\nn_t     <- 100 \n# number of subjects\nn       <- 10\n# number of hidden states\nm       <- 3\n# number of dependent variables (DV)\nn_dep   <- 1\n# number of categories for the corresponding DV\nq_emiss <- 4\n# m by m matrix containing the average population transition probabilities\ngamma   <- matrix(c(0.8, 0.1, 0.1,\n                    0.2, 0.7, 0.1,\n                    0.2, 0.2, 0.6), ncol = m, byrow = TRUE)\n# a list with n_dep number of matrices containing the average population\n# emission distribution(s)\nemiss_distr <- list(matrix(c(0.5, 0.5, 0.0, 0.0,\n                             0.1, 0.1, 0.8, 0.0,\n                             0.0, 0.0, 0.1, 0.9), nrow = m, ncol = q_emiss, byrow = TRUE))\n# simulate data using sim_mHMM\ndata1 <- sim_mHMM(n_t = n_t, n = n, gen = list(m = m, n_dep = n_dep, q_emiss = q_emiss),\n                  gamma = gamma, emiss_distr = emiss_distr, var_gamma = 1, var_emiss = 1)\n\nThe simulated data1 structures are depicted in Table 9.1 below. The following components are returned by the function sim_mHMM:\n\n\nstates: A matrix containing the simulated hidden state sequences, with one row per hidden state per subject. The first column indicates subject id number. The second column contains the simulated hidden state sequence, consecutively for all subjects. Hence, the id number is repeated over the rows (with the number of repeats equal to the length of the simulated hidden state sequence T for each subject).\n\nobs: A matrix containing the simulated observed outputs, with one row per simulated observation per subject. The first column indicates subject id number. The second column contains the simulated observation sequence, consecutively for all subjects. Hence, the id number is repeated over rows as in states.\n\n\n\nTable 9.1: data1 structure\n\n\n\n\n(a) states\n\nsubj\nstate\n\n\n\n1\n3\n\n\n1\n3\n\n\n1\n1\n\n\n1\n1\n\n\n1\n1\n\n\n1\n1\n\n\n\n\n\n\n(b) observations\n\nsubj\nobservation 1\n\n\n\n1\n4\n\n\n1\n4\n\n\n1\n2\n\n\n1\n2\n\n\n1\n2\n\n\n1\n1"
  },
  {
    "objectID": "simulate.html#simulating-data-with-a-covariate",
    "href": "simulate.html#simulating-data-with-a-covariate",
    "title": "9  Simulating data",
    "section": "\n9.2 Simulating data with a covariate",
    "text": "9.2 Simulating data with a covariate\nNow, let’s proceed to simulate data with a covariate. First, we’ll explore some additional arguments that are used when a covariate is included.\n\n\nxx_vec: List of 1 + n_dep vectors containing the covariate(s) to predict the transition probability matrix gamma and/or (specific) emission distribution(s) emiss_distr using the regression parameters specified in beta (see below). The first element in the list xx_vec is used to predict the transition matrix. Subsequent elements in the list are used to predict the emission distribution of (each of) the dependent variable(s). This means that the covariate used to predict gamma and emiss_distr can either be the same covariate, different covariates, or a covariate for certain elements and none for the other.\n\n\n\n\n\n\n\nNote\n\n\n\nFor all elements in the list, the number of observations in the vectors should be equal to the number of subjects to be simulated n. If xx_vec is omitted completely, xx_vec defaults to NULL, resembling no covariates at all. Specific elements in the list can also be left empty to signify that either the transition probability matrix or (one of) the emission distribution(s) is not predicted by covariates.\n\n\n\n\nbeta: List of 1 + n_dep matrices containing the regression parameters to predict and/or emiss_distr in combination with using (Multinomial logistic) regression. The first matrix is used to predict the transition probability matrix gamma. The subsequent matrices are used to predict the emission distribution(s) of the dependent variable(s) emiss_distr. For gamma and categorical emission distributions, one regression parameter is specified for each element in gamma and emiss_distr, with the following exceptions.\n\n\n\n\n\n\n\nExceptions\n\n\n\nIn Multinomial logistic regression, the first element in each row of gamma and/or emiss_distr serves as the reference category. Consequently, no regression parameters can be specified for these reference elements. Therefore, the first element in the beta list used to predict gamma consists of a matrix with m rows and m - 1 columns. For categorical emission distributions, the subsequent elements in the beta list used to predict emiss_distr consist of matrices with m rows and q_emiss[k] - 1 columns for each of the k in n_dep emission distribution(s). For continuous emission distributions, the subsequent elements in the beta list consist of matrices with m rows and 1 column.\n\n\n\n## 2. Including a covariate to predict (only) the transition probability matrix gamma\n# create beta list\nbeta      <- rep(list(NULL), 2)\n# first list to predict gamma\nbeta[[1]] <- matrix(c(0.5, 1.0,\n                     -0.5, 0.5,\n                      0.0, 1.0), byrow = TRUE, ncol = 2)\n# covariate list\nxx_vec      <- rep(list(NULL),2)\n# first list to predict gamma\nxx_vec[[1]] <-  c(rep(0,5), rep(1,5))\n# simulate data with a covariate\ndata2 <- sim_mHMM(n_t = n_t, n = n, gen = list(m = m, n_dep = n_dep, q_emiss = q_emiss),\n                  gamma = gamma, emiss_distr = emiss_distr, beta = beta, xx_vec = xx_vec,\n                  var_gamma = 1, var_emiss = 1)"
  },
  {
    "objectID": "simulate.html#simulating-subject-specific-gammaemission-distributions",
    "href": "simulate.html#simulating-subject-specific-gammaemission-distributions",
    "title": "9  Simulating data",
    "section": "\n9.3 Simulating subject-specific gamma/emission distribution(s)",
    "text": "9.3 Simulating subject-specific gamma/emission distribution(s)\nLastly, as mentioned above, we can also simulate the subject-specific gamma and emission distributions. There are two ways to achieve this; by either setting n_t to zero or use an argument called return_ind_par. Below, we provide examples of both methods.\n\n## 3. Simulating only the subject-specific transition probability matrices \n## and emission distributions by setting n_t = 0\nn_t <- 0 # set n_t to zero\nn <- 5\nm <- 3\nn_dep   <- 1\nq_emiss <- 4\ngamma <- matrix(c(0.8, 0.1, 0.1,\n                  0.2, 0.7, 0.1,\n                  0.2, 0.2, 0.6), ncol = m, byrow = TRUE)\nemiss_distr <- list(matrix(c(0.5, 0.5, 0.0, 0.0,\n                             0.1, 0.1, 0.8, 0.0,\n                             0.0, 0.0, 0.1, 0.9), nrow = m, ncol = q_emiss, byrow = TRUE))\n\ndata3 <- sim_mHMM(n_t = n_t, n = n, gen = list(m = m, n_dep = n_dep, q_emiss = q_emiss),\n                  gamma = gamma, emiss_distr = emiss_distr, var_gamma = 1, var_emiss = 1)\n\n\n\n\n\n\n\nSee the list of subject-specific gamma and emission distributions from the output data3\n\n\n\n\n\n\n\n#> $subject_gamma\n#> $subject_gamma[[1]]\n#>        [,1]   [,2]   [,3]\n#> [1,] 0.8932 0.0582 0.0487\n#> [2,] 0.5681 0.1780 0.2539\n#> [3,] 0.2256 0.1905 0.5839\n#> \n#> $subject_gamma[[2]]\n#>        [,1]   [,2]   [,3]\n#> [1,] 0.8344 0.1458 0.0197\n#> [2,] 0.0758 0.8803 0.0439\n#> [3,] 0.1054 0.3471 0.5474\n#> \n#> $subject_gamma[[3]]\n#>        [,1]   [,2]   [,3]\n#> [1,] 0.8326 0.0544 0.1130\n#> [2,] 0.0780 0.9047 0.0173\n#> [3,] 0.1594 0.0674 0.7732\n#> \n#> $subject_gamma[[4]]\n#>        [,1]   [,2]   [,3]\n#> [1,] 0.7326 0.1225 0.1450\n#> [2,] 0.1996 0.5575 0.2429\n#> [3,] 0.2703 0.4418 0.2880\n#> \n#> $subject_gamma[[5]]\n#>        [,1]   [,2]   [,3]\n#> [1,] 0.7516 0.1765 0.0719\n#> [2,] 0.6085 0.3243 0.0672\n#> [3,] 0.3707 0.1104 0.5189\n#> \n#> \n#> $subject_emiss\n#> $subject_emiss[[1]]\n#> $subject_emiss[[1]][[1]]\n#>        [,1]   [,2]   [,3]   [,4]\n#> [1,] 0.3471 0.6529 0.0000 0.0000\n#> [2,] 0.0288 0.0123 0.9589 0.0000\n#> [3,] 0.0000 0.0001 0.1541 0.8458\n#> \n#> \n#> $subject_emiss[[2]]\n#> $subject_emiss[[2]][[1]]\n#>        [,1]   [,2]   [,3]   [,4]\n#> [1,] 0.6365 0.3635 0.0000 0.0000\n#> [2,] 0.0575 0.3478 0.5947 0.0000\n#> [3,] 0.0000 0.0000 0.0902 0.9098\n#> \n#> \n#> $subject_emiss[[3]]\n#> $subject_emiss[[3]][[1]]\n#>        [,1]   [,2]   [,3]   [,4]\n#> [1,] 0.7638 0.2362 0.0000 0.0000\n#> [2,] 0.0576 0.1530 0.7894 0.0000\n#> [3,] 0.0000 0.0000 0.0142 0.9857\n#> \n#> \n#> $subject_emiss[[4]]\n#> $subject_emiss[[4]][[1]]\n#>        [,1]   [,2]   [,3]   [,4]\n#> [1,] 0.4130 0.5870 0.0000 0.0000\n#> [2,] 0.0534 0.0369 0.9097 0.0000\n#> [3,] 0.0000 0.0000 0.1962 0.8037\n#> \n#> \n#> $subject_emiss[[5]]\n#> $subject_emiss[[5]][[1]]\n#>        [,1]   [,2]   [,3]   [,4]\n#> [1,] 0.6560 0.3440 0.0000 0.0000\n#> [2,] 0.0441 0.0846 0.8713 0.0000\n#> [3,] 0.0000 0.0000 0.3055 0.6944\n\n\n\n\n\nThis time, we set return_ind_par to true to simulate the subject-specific transition probability matrices and emission distributions. The distinction from setting n_t to zero is that, in the latter case, it only returns the simulated subject-specific transition probability matrices and emission distributions without generating any data. However, when using return_ind_par, it not only returns the simulated subject-specific parameters but also provides the simulated data.\n\n## 4. Simulating the subject-specific transition probability matrices \n## and emission distributions by setting return_ind_par = TRUE\ndata4 <- sim_mHMM(n_t = 10, n = n, gen = list(m = m, n_dep = n_dep, q_emiss = q_emiss),\n                  gamma = gamma, emiss_distr = emiss_distr, var_gamma = .5, var_emiss =.5,\n                  return_ind_par = TRUE)\n\n\n\n\n\n\n\nSee the details of the output data4\n\n\n\n\n\n\n\n#> $states\n#>       subj state\n#>  [1,]    1     1\n#>  [2,]    1     1\n#>  [3,]    1     1\n#>  [4,]    1     1\n#>  [5,]    1     1\n#>  [6,]    1     2\n#>  [7,]    1     3\n#>  [8,]    1     1\n#>  [9,]    1     1\n#> [10,]    1     2\n#> [11,]    2     1\n#> [12,]    2     1\n#> [13,]    2     2\n#> [14,]    2     1\n#> [15,]    2     1\n#> [16,]    2     1\n#> [17,]    2     1\n#> [18,]    2     1\n#> [19,]    2     1\n#> [20,]    2     1\n#> [21,]    3     1\n#> [22,]    3     1\n#> [23,]    3     1\n#> [24,]    3     1\n#> [25,]    3     1\n#> [26,]    3     1\n#> [27,]    3     2\n#> [28,]    3     2\n#> [29,]    3     2\n#> [30,]    3     2\n#> [31,]    4     1\n#> [32,]    4     1\n#> [33,]    4     2\n#> [34,]    4     2\n#> [35,]    4     1\n#> [36,]    4     1\n#> [37,]    4     1\n#> [38,]    4     1\n#> [39,]    4     1\n#> [40,]    4     1\n#> [41,]    5     1\n#> [42,]    5     3\n#> [43,]    5     3\n#> [44,]    5     3\n#> [45,]    5     1\n#> [46,]    5     2\n#> [47,]    5     2\n#> [48,]    5     3\n#> [49,]    5     3\n#> [50,]    5     2\n#> \n#> $obs\n#>       subj observation 1\n#>  [1,]    1             2\n#>  [2,]    1             1\n#>  [3,]    1             1\n#>  [4,]    1             2\n#>  [5,]    1             2\n#>  [6,]    1             3\n#>  [7,]    1             4\n#>  [8,]    1             1\n#>  [9,]    1             1\n#> [10,]    1             3\n#> [11,]    2             1\n#> [12,]    2             1\n#> [13,]    2             3\n#> [14,]    2             1\n#> [15,]    2             2\n#> [16,]    2             2\n#> [17,]    2             1\n#> [18,]    2             1\n#> [19,]    2             1\n#> [20,]    2             1\n#> [21,]    3             2\n#> [22,]    3             2\n#> [23,]    3             1\n#> [24,]    3             1\n#> [25,]    3             1\n#> [26,]    3             2\n#> [27,]    3             3\n#> [28,]    3             3\n#> [29,]    3             3\n#> [30,]    3             3\n#> [31,]    4             1\n#> [32,]    4             1\n#> [33,]    4             3\n#> [34,]    4             3\n#> [35,]    4             2\n#> [36,]    4             2\n#> [37,]    4             2\n#> [38,]    4             2\n#> [39,]    4             2\n#> [40,]    4             1\n#> [41,]    5             1\n#> [42,]    5             3\n#> [43,]    5             4\n#> [44,]    5             4\n#> [45,]    5             1\n#> [46,]    5             3\n#> [47,]    5             3\n#> [48,]    5             4\n#> [49,]    5             4\n#> [50,]    5             3\n#> \n#> $subject_gamma\n#> $subject_gamma[[1]]\n#>        [,1]   [,2]   [,3]\n#> [1,] 0.7799 0.1663 0.0537\n#> [2,] 0.2381 0.6529 0.1090\n#> [3,] 0.3283 0.2094 0.4623\n#> \n#> $subject_gamma[[2]]\n#>        [,1]   [,2]   [,3]\n#> [1,] 0.7171 0.2009 0.0820\n#> [2,] 0.1659 0.3226 0.5115\n#> [3,] 0.3843 0.3413 0.2744\n#> \n#> $subject_gamma[[3]]\n#>        [,1]   [,2]   [,3]\n#> [1,] 0.8642 0.0827 0.0531\n#> [2,] 0.1185 0.6883 0.1932\n#> [3,] 0.2104 0.4419 0.3477\n#> \n#> $subject_gamma[[4]]\n#>        [,1]   [,2]   [,3]\n#> [1,] 0.8437 0.0816 0.0746\n#> [2,] 0.1546 0.7859 0.0595\n#> [3,] 0.1639 0.5371 0.2990\n#> \n#> $subject_gamma[[5]]\n#>        [,1]   [,2]   [,3]\n#> [1,] 0.5605 0.2586 0.1809\n#> [2,] 0.1185 0.6510 0.2305\n#> [3,] 0.1803 0.1913 0.6284\n#> \n#> \n#> $subject_emiss\n#> $subject_emiss[[1]]\n#> $subject_emiss[[1]][[1]]\n#>        [,1]   [,2]   [,3]   [,4]\n#> [1,] 0.5200 0.4800 0.0000 0.0000\n#> [2,] 0.0329 0.0194 0.9476 0.0000\n#> [3,] 0.0000 0.0000 0.0329 0.9671\n#> \n#> \n#> $subject_emiss[[2]]\n#> $subject_emiss[[2]][[1]]\n#>        [,1]   [,2]   [,3]   [,4]\n#> [1,] 0.7685 0.2314 0.0000 0.0001\n#> [2,] 0.0368 0.0862 0.8769 0.0000\n#> [3,] 0.0000 0.0000 0.0241 0.9759\n#> \n#> \n#> $subject_emiss[[3]]\n#> $subject_emiss[[3]][[1]]\n#>        [,1]   [,2]   [,3]   [,4]\n#> [1,] 0.3646 0.6354 0.0000 0.0000\n#> [2,] 0.2471 0.1230 0.6299 0.0000\n#> [3,] 0.0000 0.0000 0.0933 0.9067\n#> \n#> \n#> $subject_emiss[[4]]\n#> $subject_emiss[[4]][[1]]\n#>        [,1]   [,2]   [,3]   [,4]\n#> [1,] 0.3169 0.6831 0.0000 0.0000\n#> [2,] 0.0906 0.0376 0.8718 0.0000\n#> [3,] 0.0000 0.0000 0.0485 0.9515\n#> \n#> \n#> $subject_emiss[[5]]\n#> $subject_emiss[[5]][[1]]\n#>        [,1]   [,2]   [,3]   [,4]\n#> [1,] 0.6358 0.3641 0.0000 0.0000\n#> [2,] 0.0854 0.0435 0.8711 0.0000\n#> [3,] 0.0000 0.0000 0.1317 0.8683"
  },
  {
    "objectID": "covariate.html",
    "href": "covariate.html",
    "title": "10  Using covariates",
    "section": "",
    "text": "In Chapter 8, we show how to fit a multilevel hidden Markov model using the mHMM function. Here, we extend the scenario by including a covariate. When utilizing a covariate, we need to use xx argument in the mHMM function.\n\n\nxx: An optional list of (level 2) covariates to predict the transition matrix and/or the emission probabilities. Level 2 covariate(s) means that there is one observation per subject of each covariate. The first element in the list xx is used to predict the transition matrix. Subsequent elements in the list are used to predict the emission distribution of (each of) the dependent variable(s). Each element in the list is a matrix, with the number of rows equal to the number of subjects. The first column of each matrix represents the intercept, that is, a column only consisting of ones. Subsequent columns correspond to covariates used to predict the transition matrix / emission distribution.\n\n\n\n\n\n\n\nNote\n\n\n\nCovariates specified in xx can either be dichotomous or continuous variables. Dichotomous variables have to be coded as 0/1 variables. Categorical or factor variables can as yet not be used as predictor covariates. The user can however break up the categorical variable in multiple dummy variables (i.e., dichotomous variables), which can be used simultaneously in the analysis. Continuous predictors are automatically centered. This is done such that the presented probabilities in the output correspond to the predicted probabilities at the average value of the covariate(s).\n\n\nAs an illustrative example, we fit a multilevel HMM that includes covariates using the nonverbal_cov dataset (see ?nonverbal_cov). Since this is just for demonstration purposes, the MCMC iterations (J) and burn-in period (burn_in) are set to very small values; 10 and 5, respectively.\n\n## Run a model including covariates to predict the gamma & emission distributions for each of the 4 dependent variables:\n\n# specifying general model properties:\nm <- 2\nn_dep <- 4\nq_emiss <- c(3, 2, 3, 2)\nn_subj <- 10\n\n# specifying starting values\nstart_TM <- diag(.8, m)\nstart_TM[lower.tri(start_TM) | upper.tri(start_TM)] <- .2\nstart_EM <- list(matrix(c(0.05, 0.90, 0.05,\n                          0.90, 0.05, 0.05), byrow = TRUE,\n                        nrow = m, ncol = q_emiss[1]), # vocalizing patient\n                 matrix(c(0.1, 0.9,\n                          0.1, 0.9), byrow = TRUE, nrow = m,\n                        ncol = q_emiss[2]), # looking patient\n                 matrix(c(0.90, 0.05, 0.05,\n                          0.05, 0.90, 0.05), byrow = TRUE,\n                        nrow = m, ncol = q_emiss[3]), # vocalizing therapist\n                 matrix(c(0.1, 0.9,\n                          0.1, 0.9), byrow = TRUE, nrow = m,\n                        ncol = q_emiss[4])) # looking therapist\n\n# specifying covariates for gamma (xx_gamma) & emission distributions (xx_emiss)\nxx_gamma <- rep(list(matrix(c(rep(1, n_subj), nonverbal_cov$` std_SCA_change`),\n                            ncol = 2, nrow = n_subj)))\nxx_emiss <- rep(list(matrix(c(rep(1, n_subj), nonverbal_cov$std_CDI_change, \n                     nonverbal_cov$diagnosis), ncol = 3, nrow = n_subj)), n_dep)\nxx <- c(xx_gamma, xx_emiss)\n\n# fit the model\nout_2st_c <- mHMM(s_data = nonverbal, xx = xx,\n                  gen = list(m = m, n_dep = n_dep, q_emiss = q_emiss),\n                  start_val = c(list(start_TM), start_EM),\n                  mcmc = list(J = 10, burn_in = 5))\n\nWe can again check the result by using the print and summary functions.\n\nprint(out_2st_c)\n\n#> Number of subjects: 10 \n#> \n#> 10 iterations used in the MCMC algorithm with a burn in of 5 \n#> Average Log likelihood over all subjects: -1638.586 \n#> Average AIC over all subjects: 3305.172 \n#> \n#> Number of states used: 2 \n#> \n#> Number of dependent variables used: 4\n\n\nWhen covariates are used, the summary function also returns regression coefficients (\\(\\beta\\)) along with their credible intervals. See below for an example of the output.\n\nsummary(out_2st_c)\n\n\n\n\n\n\n\nSee the summary output when covariates are included: summary(out_2st_c)\n\n\n\n\n\n\n\n#> State transition probability matrix \n#>  (at the group level): \n#>  \n#>              To state 1 To state 2\n#> From state 1      0.915      0.085\n#> From state 2      0.059      0.941\n#> \n#> Regression coefficients predicting the transition probabilities \n#>  (at the group level): \n#>  \n#> $cov1\n#>  From_state To_state   Beta CrI_lower CrI_upper  \n#>           1        2  0.034    -0.476     0.379  \n#>           2        2 -0.253    -0.533    -0.161 *\n#> \n#> Note: [*] 95% credible interval does not include zero.\n#>  \n#> Emission distribution for each of the dependent variables \n#>  (at the group level): \n#>  \n#> $p_vocalizing\n#>         Category 1 Category 2 Category 3\n#> State 1      0.027      0.929      0.044\n#> State 2      0.741      0.114      0.145\n#> \n#> $p_looking\n#>         Category 1 Category 2\n#> State 1      0.284      0.716\n#> State 2      0.110      0.890\n#> \n#> $t_vocalizing\n#>         Category 1 Category 2 Category 3\n#> State 1      0.778      0.069      0.153\n#> State 2      0.038      0.941      0.021\n#> \n#> $t_looking\n#>         Category 1 Category 2\n#> State 1      0.053      0.947\n#> State 2      0.231      0.769\n#> \n#> \n#> Regression coefficients predicting the emission probabilities \n#>  for each of the dependent variables \n#>  (at the group level): \n#>  \n#> $cov1\n#> $cov1$p_vocalizing\n#>  Category State   Beta CrI_lower CrI_upper  \n#>         2     1  0.057    -0.337     0.457  \n#>         3     1 -0.437    -0.954     0.231  \n#>         2     2 -0.145    -0.255     0.219  \n#>         3     2 -0.292    -0.729     0.073  \n#> \n#> $cov1$p_looking\n#>  Category State   Beta CrI_lower CrI_upper  \n#>         2     1 -0.682    -0.969    -0.013 *\n#>         2     2 -0.696    -1.287     0.006  \n#> \n#> $cov1$t_vocalizing\n#>  Category State   Beta CrI_lower CrI_upper  \n#>         2     1  0.103    -0.680     0.916  \n#>         3     1  0.229    -0.161     0.311  \n#>         2     2  0.032    -0.190     0.400  \n#>         3     2 -0.679    -0.904    -0.082 *\n#> \n#> $cov1$t_looking\n#>  Category State   Beta CrI_lower CrI_upper  \n#>         2     1 -0.125    -0.411     0.942  \n#>         2     2 -0.060    -0.340     0.042  \n#> \n#> \n#> $cov2\n#> $cov2$p_vocalizing\n#>  Category State   Beta CrI_lower CrI_upper  \n#>         2     1  0.281    -0.311     0.476  \n#>         3     1  0.437    -0.555     0.702  \n#>         2     2 -0.102    -0.988     0.014  \n#>         3     2  0.204     0.118     0.466 *\n#> \n#> $cov2$p_looking\n#>  Category State  Beta CrI_lower CrI_upper  \n#>         2     1 0.060    -0.308     0.521  \n#>         2     2 0.836    -0.608     1.729  \n#> \n#> $cov2$t_vocalizing\n#>  Category State   Beta CrI_lower CrI_upper  \n#>         2     1  0.390     0.163     1.338 *\n#>         3     1 -0.617    -1.431    -0.346 *\n#>         2     2  0.649    -1.265     1.398  \n#>         3     2  0.356    -0.161     0.968  \n#> \n#> $cov2$t_looking\n#>  Category State   Beta CrI_lower CrI_upper  \n#>         2     1  0.440     -1.32     0.559  \n#>         2     2 -0.321     -0.93     0.250  \n#> \n#> \n#> Note: [*] 95% credible interval does not include zero.\n\n\n\n\n\n\n\n\n\n\n\nInterpretation of coefficients (\\(\\beta\\))\n\n\n\nThe interpretation of regression coefficients in the context of a multilevel hidden Markov model (HMM) with covariates can depend on the specific application and the nature of the covariates used. In general, the interpretation of regression coefficients can be similar to that in traditional regression models, but with some additional considerations due to the hidden Markov nature of the data.\n\nTransition Probability Matrix: In the case of covariates influencing the transition probability matrix (gamma), the regression coefficients represent the change in the log-odds of transitioning from one state to another, for a one-unit change in the covariate. The coefficients can be positive or negative, indicating an increase or decrease in the likelihood of transitioning between states based on the covariate value.\nEmission Probabilities: For categorical emission distributions, the regression coefficients represent the log-odds of the probability of observing a particular category of the dependent variable, given a one-unit change in the covariate. Positive coefficients indicate an increase in the log-odds of observing a specific category, while negative coefficients indicate a decrease."
  },
  {
    "objectID": "plotting.html#posterior-densities-plot",
    "href": "plotting.html#posterior-densities-plot",
    "title": "11  Graphically displaying outcomes",
    "section": "\n11.1 Posterior densities plot",
    "text": "11.1 Posterior densities plot\nOne can plot the posterior densities of a fitted model, for both the transition probability matrix gamma and for the emission distribution probabilities. These densities are displayed for both the group level and the subject level simultaneously. When the ggplot2 package is installed, the functions will utilize ggplot by default. However, if ggplot2 is not available, the base R plot function will be used instead.\nHere’s an example demonstrating how to create a plot of the emission distribution for the variable p_vocalizing:\n\nlibrary(RColorBrewer)\n# Specify color and label\nVoc_col <- c(brewer.pal(3,\"PuBuGn\")[c(1,3,2)])\nVoc_lab <- c(\"Not Speaking\", \"Speaking\", \"Back channeling\")\n\n# Use base R plot function\nplot(out_2st, component = \"emiss\", dep = 1, col = Voc_col, \n     dep_lab = c(\"Patient vocalizing\"), cat_lab = Voc_lab)\n\n\n\n\n\n\n\n\n# When ggplot2 is available\nlibrary(ggplot2)\nplot(out_2st, component = \"emiss\", dep = 1, col = brewer.pal(3, \"Accent\"),\n     dep_lab = c(\"Patient vocalizing\"), cat_lab = Voc_lab)\n\n\n\n\n\n\n\n\n\nIn the plots above, the solid line visualizes the posterior density at the group level, while each of the dotted lines visualizes the posterior density of one subject.\n\n\n\n\n\n\nArguments for plot function\n\n\n\n\n\ncomponent specifies whether we want to visualize the posterior densities for the transition probability matrix gamma (component = \"gamma\") or for the emission distribution probabilities (component = \"emiss\").\nWhen using component = \"emiss\", the argument dep specifies which dependent variable we want to inspect. Here, we set dep = 1 as the variable p_vocolizing is the first variable in the set.\n\ncol specifies the colors to be used when plotting the lines.\n\ndep_lab denotes the label of the dependent variable we are plotting.\n\ncat_lab denotes the labels of the categorical outcomes in the corresponding dependent variable.\nFor more detailed information, check the help file of the function (?plot.mHMM)."
  },
  {
    "objectID": "plotting.html#transition-probabilities-plot-riverplot",
    "href": "plotting.html#transition-probabilities-plot-riverplot",
    "title": "11  Graphically displaying outcomes",
    "section": "\n11.2 Transition probabilities plot (riverplot)",
    "text": "11.2 Transition probabilities plot (riverplot)\nAdditionally, the package provides a way to plot the transition probabilities obtained using the function obtain_gamma in the form of a riverplot. The visualization showcases the transitions between hidden states in a visually appealing manner. Just like with the other plot function above, this function will automatically choose between base R plot or ggplot functions based on the presence of the ggplot2 package in your R environment.\nBelow, we provide examples of how to create riverplots (when ggplot2 is available).\n\n# Transition probabilities at the group level\ngamma_pop <- obtain_gamma(out_2st)\n# Transition probabilities for the subject number 1\ngamma_subj <- obtain_gamma(out_2st, level = \"subject\")\nplot(gamma_pop, col = brewer.pal(2, \"Set2\")) # specify color\nplot(gamma_subj, subj_nr = 1) # specify subj_nr\n\n\n\n\n\nFigure 11.1: At group level\n\n\n\n\n\n\nFigure 11.2: For subject number 1\n\n\n\n\n\n\nGraphically displaying the transition probabilities becomes more informative as the number of hidden states increases. See an example plot below that demonstrates this with four specified hidden states. As the number of states increases, the visual representation becomes more intricate!"
  },
  {
    "objectID": "plotting.html#prediction-plot-with-covariates",
    "href": "plotting.html#prediction-plot-with-covariates",
    "title": "11  Graphically displaying outcomes",
    "section": "\n11.3 Prediction plot with covariates",
    "text": "11.3 Prediction plot with covariates\nWhen covariates are included, we have the option to visualize the predicted transition probabilities and emission probabilities conditioned on covariate values using the plot_pred function. This visualization overlays the predicted probabilities on top of the probabilities per subject (dots ● in the plot), allowing us to understand how much the covariate can explain the variability in these probabilities.\nDepending on the type of covariates (dichotomous or continuous), plot_pred generates either boxplots or line plots. It’s important to note that you can only plot one covariate at a time. You can specify which covariate you want to plot using the cov argument, especially when there are multiple covariates available. See below for the detailed arguments used in the plot_pred function:\n\n\n\n\n\n\nArguments for plot_pred function\n\n\n\n\n\nobject: Object of class mHMM.\n\ncomponent: String specifying if the plot is made for the transition probability matrix gamma (default: component = \"gamma\") or for the emission distribution probabilities (component = \"emiss\").\n\ncov: Integer that specifies which covariate to use for plotting when there are multiple covariates available. The default value is 1, which indicates the first covariate in the list of covariates.\n\ndep: Integer specifying for which dependent variable the predicted emission probabilities are plotted.\n\ncol: Vector of colors for the plots. If one is plotting the predicted transition probabilities, the vector has length m. If one is plotting the predicted emission probabilities, the vector has length q_emiss[k]. If not specified, colors will be assigned automatically.\n\ncat_lab: Optional vector of strings when plotting the predicted emission probabilities, denoting the labels of the categorical outcome values. Automatically generated when not provided.\n\ndep_lab: Optional string when plotting the predicted the emission probabilities with length 1, denoting the label for the dependent variable plotted. Automatically obtained from the input object when not specified.\n\ncov_lab:Optional string denoting the label for the covariate variable plotted. Automatically generated when not provided.\n\n\n\n\n# Plot of predicted gamma vs covariate \nplot_pred(out_2st_c, cov = 1, col = brewer.pal(2, \"Accent\"),  \n          cov_lab = \"change in anxiety after therapy\") \n\n\n\n\n\n\n\n\n# Plot of predicted emission prob vs covariate\nplot_pred(out_2st_c, cov = 1, component = \"emiss\", dep_lab = \"patient vocalizing\", \n          col = brewer.pal(3, \"Accent\"))\nplot_pred(out_2st_c, cov = 2, component = \"emiss\", cat_lab = \n            c(\"Not Speaking\", \"Speaking\", \"Back channeling\"), col = brewer.pal(3, \"Set2\"))"
  },
  {
    "objectID": "hiddenstate.html",
    "href": "hiddenstate.html",
    "title": "12  Determining the number of hidden states",
    "section": "",
    "text": "The first step in developing a HMM is to determine the number of states \\(m\\) that best describes the observed data, and is a model selection problem. When modelling, for example, behavior, the task is to define the states by clusters of observed behavioral outcomes that provide a reasonable, theoretically interpretable, description of the data.\nWe suggest using a combination of the Akaike Information Criterion (AIC) and the theoretical interpretability of the estimated states to choose between models1. In the example dataset, the 2-, 3- and 4-state model result in an AIC of 3279, 3087, and 2959, respectively. According to model fit indices, the 4 -state model is clearly the best model2. Let’s inspect the composition of the states for the 4 state model, and the transition probabilities.\nWe can see that we have a state in which the patient speaks and the therapist is silent (state 1), a state in which the patient is silent and the therapist speaks (state 2), a state in which both the patient and therapist speak (state 3) and a state in which the therapist speaks but does not look at the patient (in contrast to the looking behavior in all other states), and the patient is silent. In addition, all states are quite stable as the probability of remaining in the same state is above .6 for all states.\n\nsummary(out_4st)\n\n#> State transition probability matrix \n#>  (at the group level): \n#>  \n#>              To state 1 To state 2 To state 3 To state 4\n#> From state 1      0.910      0.031      0.040      0.019\n#> From state 2      0.044      0.815      0.056      0.084\n#> From state 3      0.183      0.097      0.666      0.054\n#> From state 4      0.024      0.220      0.019      0.738\n#> \n#> Emission distribution for each of the dependent variables \n#>  (at the group level): \n#>  \n#> $p_vocalizing\n#>         Category 1 Category 2 Category 3\n#> State 1      0.011      0.976      0.013\n#> State 2      0.729      0.046      0.225\n#> State 3      0.184      0.665      0.150\n#> State 4      0.876      0.065      0.059\n#> \n#> $p_looking\n#>         Category 1 Category 2\n#> State 1      0.237      0.763\n#> State 2      0.061      0.939\n#> State 3      0.439      0.561\n#> State 4      0.094      0.906\n#> \n#> $t_vocalizing\n#>         Category 1 Category 2 Category 3\n#> State 1      0.889      0.013      0.097\n#> State 2      0.019      0.971      0.011\n#> State 3      0.332      0.593      0.074\n#> State 4      0.087      0.845      0.068\n#> \n#> $t_looking\n#>         Category 1 Category 2\n#> State 1      0.030      0.970\n#> State 2      0.045      0.955\n#> State 3      0.074      0.926\n#> State 4      0.949      0.051\n\nplot(obtain_gamma(out_4st))\n\n\n\n\n\n\n\n\n\n\n\nRydén, Tobias. 2008. “EM Versus Markov Chain Monte Carlo for Estimation of Hidden Markov Models: A Computational Perspective.” Bayesian Analysis 3 (4): 659–88.\n\n\nScott, Steven L. 2002. “Bayesian Methods for Hidden Markov Models.” Journal of the American Statistical Association 97 (457).\n\n\n\n\nNote that the likelihood ratio test, commonly used to compare nested models, cannot be used in case of the HMM (i.e., the difference in the log-likelihoods between models is not \\(\\chi^2\\) distributed Rydén (2008)).↩︎\nWe note, however, that the AIC approximates the posterior distribution of the parameters by a Gaussian distribution, which might not be appropriate for models including parameters on the boundary of the parameter space (e.g., close to 0 or 1 in case of probability estimates), or for small data sets, as exemplified by Scott (2002). Model selection is therefore not a straightforward procedure in the context of HMM, and the choices remain subjective.↩︎"
  },
  {
    "objectID": "sequence.html",
    "href": "sequence.html",
    "title": "13  Determining the most likely state sequence",
    "section": "",
    "text": "Given a well-fitting HMM, it may be of interest to determine the actual sequence, or order of succession, of hidden states that has most likely given rise to the sequence of outcomes as observed in a subject. One can either use local decoding, in which the probabilities of the hidden state sequence are obtained simultaneously with the model parameters estimates, or the well-known Viterbi algorithm (Viterbi 1967; Forney Jr 1973). In local decoding, the most likely state is determined separately at each time point \\(t\\), in contrast to the Viterbi algorithm in which one determines the joint probability of the complete sequence of observations \\(O_{1:T}\\) and the complete sequence of hidden states \\(S_{1:T}\\).\nIn the package, local decoding can be achieved by saving the sampled hidden state sequence at each iteration of the Gibbs sampler, by setting the input variable return_path = TRUE for the function mHMM. This will result in very large output files, however. Global decoding can be performed by using the function vit_mHMM:\n\nstate_seq <- vit_mHMM(out_2st, s_data = nonverbal)\nhead(state_seq)\n\n#>      Subj_1 Subj_2 Subj_3 Subj_4 Subj_5 Subj_6 Subj_7 Subj_8 Subj_9 Subj_10\n#> [1,]      1      2      2      2      1      2      1      1      1       1\n#> [2,]      1      2      2      2      1      2      1      1      1       1\n#> [3,]      1      2      2      2      2      2      1      1      1       1\n#> [4,]      1      2      2      2      2      2      1      1      1       1\n#> [5,]      1      2      2      2      2      2      2      1      1       1\n#> [6,]      1      2      2      2      2      1      2      1      1       1\n\n\nThe function returns the hidden state sequence for each subject in a matrix, where each row represents a point in time and each column represents a subject. We can inspect the obtained hidden state sequence by for example plotting it together with the observed data. Below, the first 5 minutes of the first couple is plotted again, with the addition of the estimated state sequence:\n\n\n\n\n\n\n\n\n\n\n\n\nForney Jr, G David. 1973. “The Viterbi Algorithm.” Proceedings of the IEEE 61 (3): 268–78.\n\n\nViterbi, Andrew J. 1967. “Error Bounds for Convolutional Codes and an Asymptotically Optimum Decoding Algorithm.” Information Theory, IEEE Transactions on 13 (2): 260–69."
  },
  {
    "objectID": "convergence.html",
    "href": "convergence.html",
    "title": "14  Model convergence and label switching",
    "section": "",
    "text": "When employing Bayesian estimation procedures, it is crucial to ensure model convergence and address label switching issues. Model convergence refers to checking whether the algorithm reaches a consistent solution when different starting values are utilized (but conceptually similar ones). On the other hand, label switching occurs when the ordering of the states switches during the estimation iterations. For example, the state initially labeled as 1 may later become state 2.\nTo visually inspect model convergence and label switching, one can utilize traceplots of parameters from identical models with varying starting values. These traceplots display the sampled parameter values over the iterations and can be generated using the traceplot function. Below, we demonstrate how this is done using the 2-state models with different starting values.\n\n# specifying general model properties\nm <-2\nn_dep <- 4\nq_emiss <- c(3, 2, 3, 2)\n# specifying different starting values\nstart_TM <- diag(.8, m)\nstart_TM[lower.tri(start_TM) | upper.tri(start_TM)] <- .2\nstart_EM_b <- list(matrix(c(0.2, 0.6, 0.2,\n                            0.6, 0.2, 0.2), byrow = TRUE,\n                        nrow = m, ncol = q_emiss[1]), # vocalizing patient\n                 matrix(c(0.4, 0.6,\n                          0.4, 0.6), byrow = TRUE, nrow = m,\n                        ncol = q_emiss[2]), # looking patient\n                 matrix(c(0.6, 0.2, 0.2,\n                          0.2, 0.6, 0.2), byrow = TRUE,\n                        nrow = m, ncol = q_emiss[3]), # vocalizing therapist\n                 matrix(c(0.4, 0.6,\n                          0.4, 0.6), byrow = TRUE, nrow = m,\n                        ncol = q_emiss[4])) # looking therapist\n\n# run a model identical to out_2st, but with different starting values:\nset.seed(9843)\nout_2st_b <- mHMM(s_data = nonverbal, \n                      gen = list(m = m, n_dep = n_dep, q_emiss = q_emiss), \n                      start_val = c(list(start_TM), start_EM),\n                      mcmc = list(J = 1000, burn_in = 200))\n\nIf we, for example, want to inspect the traceplots for the transition probabilities, we can do the following:\n\n# two models with different starting values\nL <- list(out_2st, out_2st_b)\ntraceplot(L)\n\n\n\nFigure 14.1: Traceplot of transition probabilities\n\n\n\n\nIf we want to inspect emission probabilities for looking behavior of the patient at the group level, we can use the following code:\n\ntraceplot(L, component = \"emiss\", dep = 2, dep_lab = \"patient looking behavior\", \n          cat_lab = c(\"not looking at therapist\", \"looking at therapist\"), \n          col = brewer.pal(2, name = \"Set2\"))\n\n\n\nFigure 14.2: Traceplot of emission probabilities\n\n\n\n\n\n\n\n\n\n\nArguments for traceplot function\n\n\n\ntraceplot creates the traceplot(s) for the group-level parameter estimates corresponding to a fitted multilevel hidden Markov model. It takes the following arguments:\n\n\nL: list of objects of class mHMM.\n\ncomponent: string is used to specify whether the traceplot should be created for the parameter estimates of the transition probabilities component = \"gamma\" or the emission probabilities component = \"emiss\". Default is component = \"gamma\".\n\ndep: integer specifying for which dependent variable the traceplot should be plotted. Only required if one wishes to plot the emission distribution probabilities.\n\ncol: optional vector of colors for the traceplot lines. The vector should have a length equal to the number of mHMM objects in the list L.\n\ncat_lab: optional vector of strings denoting the labels of the categorical outcome values.\n\ndep_lab: optional string when plotting for the emission probabilities, denoting the label for the dependent variable plotted.\n\nburn_in: optional integer which specifies the number of iterations to discard.\n\n\n\nIn the figures Figure 14.1 and Figure 14.2, we can observe that the parameter estimates converge to the same parameter space, indicating a stable estimation. Additionally, the chains appear to mix well, showing smooth and consistent movements throughout the iterations. Furthermore, there is no evidence of label switching. These are positive signs that the model has converged to a consistent solution."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Altman, Rachel MacKay. 2007. “Mixed Hidden Markov\nModels: An Extension of the Hidden Markov Model to the\nLongitudinal Data Setting.” Journal of the American\nStatistical Association 102 (477): 201–10.\n\n\nBurge, Christopher B, and Samuel Karlin. 1998. “Finding the Genes\nin Genomic DNA.” Current Opinion in Structural Biology 8\n(3): 346–54.\n\n\nCappé, E. AND Rydén, O. AND Moulines. 2005. Inference in Hidden\nMarkov Models. New York: Springer.\n\n\nEphraim, Yariv, and Neri Merhav. 2002. “Hidden Markov\nProcesses.” Information Theory, IEEE Transactions on 48\n(6): 1518–69.\n\n\nForney Jr, G David. 1973. “The Viterbi\nAlgorithm.” Proceedings of the IEEE 61 (3): 268–78.\n\n\nHaan-Rietdijk, S de, Peter Kuppens, Cindy S Bergeman, LB Sheeber, NB\nAllen, and EL Hamaker. 2017. “On the Use of Mixed Markov Models\nfor Intensive Longitudinal Data.” Multivariate Behavioral\nResearch 52 (6): 747–67.\n\n\nHenderson, John, Steven Salzberg, and Kenneth H Fasman. 1997.\n“Finding Genes in DNA with a Hidden Markov Model.”\nJournal of Computational Biology 4 (2): 127–41.\n\n\nKrogh, Anders, I Saira Mian, and David Haussler. 1994. “A Hidden\nMarkov Model That Finds Genes in e. Coli DNA.” Nucleic Acids\nResearch 22 (22): 4768–78.\n\n\nRabiner, Lawrence R. 1989. “A Tutorial on Hidden\nMarkov Models and Selected Applications in Speech\nRecognition.” Proceedings of the IEEE 77 (2): 257–86.\n\n\nRueda, Oscar M, Cristina Rueda, and Ramon Diaz-Uriarte. 2013. “A\nBayesian HMM with Random Effects and an\nUnknown Number of States for DNA Copy Number\nAnalysis.” Journal of Statistical Computation and\nSimulation 83 (1): 82–96.\n\n\nRydén, Tobias. 2008. “EM Versus Markov\nChain Monte Carlo for Estimation of Hidden\nMarkov Models: A Computational Perspective.”\nBayesian Analysis 3 (4): 659–88.\n\n\nScott, Steven L. 2002. “Bayesian Methods for Hidden\nMarkov Models.” Journal of the American\nStatistical Association 97 (457).\n\n\nShirley, Kenneth E, Dylan S Small, Kevin G Lynch, Stephen A Maisto, and\nDavid W Oslin. 2010. “Hidden Markov Models for\nAlcoholism Treatment Trial Data.” The Annals of Applied\nStatistics, 366–95.\n\n\nViterbi, Andrew J. 1967. “Error Bounds for Convolutional Codes and\nan Asymptotically Optimum Decoding Algorithm.” Information\nTheory, IEEE Transactions on 13 (2): 260–69.\n\n\nWoodland, Philip C, and Daniel Povey. 2002. “Large Scale\nDiscriminative Training of Hidden Markov Models for Speech\nRecognition.” Computer Speech & Language 16 (1):\n25–47.\n\n\nZhang, Yue, and Kiros Berhane. 2014. “Bayesian Mixed Hidden\nMarkov Models: A Multi-Level Approach to Modeling\nCategorical Outcomes with Differential Misclassification.”\nStatistics in Medicine 33 (8): 1395–1408.\n\n\nZucchini, Walter, Iain L MacDonald, and Roland Langrock. 2016.\nHidden Markov Models for Time Series: An Introduction\nUsing R. Boca Raton: CRC Press."
  }
]